---
title: "HW 5"
author: "Malvika Venkataraman"
date: "11/19/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
  echo = T,
  warning = F,
  fig.width = 8,
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot.continuous.colour = "viridis",
  ggplot.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal())
```

# Problem 1

For this problem, I'm using data from the Washington Post that contains information on homicides in 50 large U.S. cities.
```{r echo=TRUE, message=FALSE, warning=FALSE}
homicide_df_raw =
  read_csv("./data/homicide-data.csv")

homicide_df_raw
```
The raw data contains information on the date, victim name, race, age, sex, the city, the state, the longitude and latitude coordinates and the disposition. The raw data has `r nrow(homicide_df_raw)` rows and `r ncol(homicide_df_raw)` columns.

First, I'm going to create a city state variable, and clean the data a little.
```{r echo=TRUE, message=FALSE, warning=FALSE}
homicide_df =
  read_csv("./data/homicide-data.csv", na = c("", "Unknown")) %>%
  mutate(city_state = str_c(city, state),
         resolution = case_when(
           disposition == "Closed without arrest" ~ "unsolved",
           disposition == "Open/No arrest" ~ "unsolved",
           disposition == "Closed by arrest" ~ "solved",
         )) %>%
  relocate(city_state) %>%
  filter(city_state != "TulsaAL")
```

I'll then use the updated dataframe to summarize within cities to obtain the total number of homicides and the number or unsolved homicides.
```{r}
#those for which the disposition is “Closed without arrest” or “Open/No arrest”
homicide_df %>%
  group_by(city_state) %>%
  summarize(
    unsolved = sum(resolution == "unsolved"),
    n = n()
  )
```

For the city of Baltimore, MD, I'll to estimate the proportion of homicides that are unsolved, as well as the confidence intervals from the resulting dataframe.
```{r}
#focus on baltimore
baltimore_df = 
  homicide_df %>%
  filter(city_state == "BaltimoreMD")

baltimore_summary = 
  baltimore_df %>%
  summarize(
    unsolved = sum(resolution == "unsolved"),
    n = n()
  )

baltimore_test =
  prop.test(
    x = baltimore_summary %>% pull(unsolved), 
    n = baltimore_summary %>% pull(n))

baltimore_test %>%
  broom::tidy() %>%
  select(estimate, starts_with("conf"))
```

I'm now going to do this for all the cities in my dataset. 
```{r}
#write a function to iterate across cities
prop_test_function = function(city_df) {
  city_summary =
    city_df %>%
    summarize(
      unsolved = sum(resolution == "unsolved"),
      n = n()
    )
  
  city_test =
    prop.test(
      x = city_summary %>% pull(unsolved),
      n = city_summary %>% pull(n))
  
  return(city_test)
}

```

```{r}
#iterate across all cities
results_df =
  homicide_df %>%
  nest(data = uid:resolution) %>%
  mutate(
    test_results = map(data, prop_test_function),
    tidy_results = map(test_results, broom::tidy)
  ) %>%
  select(city_state, tidy_results) %>%
  unnest(tidy_results) %>%
  select(city_state, estimate, starts_with("conf"))

results_df
```
I'll now create a plot that shows the estimates and confidence intervals for each city
```{r}
results_df %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  #organize cities according to the proportion of unsolved homicides
  theme(axis.text.x = element_text(angle = 90, vjust = .5, hjust = 1)) +
  labs(title = "Proportion of Unsolved Homicides and Confidence Intervals by City", 
       x = "City and State",
       y = "Estimate of Unsolved Homicides")
```

The graph shows that Richmond, VA has the lowest proportion of unsolved homicides, and that Chicago, IL has the highest proportion of unsolved homicides.

# Problem 2

For this problem, I'll be examining data from a longitudinal study that included a control arm and an experimental arm. I first need to create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time.
```{r echo=TRUE, message=FALSE, warning=FALSE}
#dataframe containing all file names
files_df = 
  tibble(
    path = list.files("zip_data")
  )

#iterate over file names and read in data from each subject
files_df =
  files_df %>%
  mutate(
    path = str_c("zip_data/", path),
    study_df = map(.x = path, ~read_csv(.x))
  ) %>%
  unnest(study_df) %>%
  #tidy the result
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "observation") %>%
  mutate(
    week = as.numeric(str_replace(week, "week_", "")),
    arm = str_extract(path, "/[a-z][a-z][a-z]"),
    arm = str_remove(arm, "/"),
    id = str_extract(path, "[0-9]+")
  ) %>%
  select(-path)

files_df
```

I will then make a spaghetti plot to show observations on each subject over the 8 week study period.
```{r}
files_df %>%
  ggplot(aes(x = week, y = observation, color = id, linetype = arm)) +
  geom_line() + 
  labs(
    title = "Observation of Control and Experimental Groups Over 8 Weeks",
    x = "Week",
    y = "Observation")
```

The graph clearly shows that subjects in the experimental arm had higher observation measurements that subjects in the control arm. By week 7, there is a clear separation between the two groups.

# Problem 3

For this problem, I'll start with a code chunk that loads data from the iris dataset and introduces some missing values in each column.
```{r}
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))

#view dataset
#View(iris_with_missing)
```

I'll fill in the missing values
```{r}
#function to replace missing values
replace_missing_vals = function(column) {
  #for numeric, fill in with the mean of non-missing values
  if (is.numeric(column)) {
    column = ifelse(is.na(column), mean(column, na.rm = T), column)
    return(column)
    }
  #for character, fill in with "virginica"
  if (is.character(column)) {
    column = ifelse(is.na(column), "virginica", column)
    return(column)
    }
}
```

I will then apply to function to the columns of the initial dataframe
```{r}
#replace missing values
new_iris = 
  iris_with_missing %>%
  map_df(~replace_missing_vals(.x))

new_iris

#view df
#View(new_iris)
```


